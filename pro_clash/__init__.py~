from subprocess import call
import logging
from tempfile import NamedTemporaryFile
from collections import defaultdict
import csv
import sys
from Bio.Seq import Seq
import numpy as np
from scipy.stats import fisher_exact
import glob
import gzip


from . import ecocyc_parser

# Functions for simple mapping (single fragment)
def run_bwa(bwa_cmd, fname1, fname2, output_dir, output_prefix, mismatches,
            fasta_genome, params_aln, params_sampe, params_samse, samtools_cmd):
    """
    Run bwa on paired or single end fastq files. write a sorted  bam and a bai
    file
    Arguments:
    - `bwa_cmd`: executable of bwa
    - `fname1`: The R1 fastq file
    - `fname2`: The R2 fastq file
    - `output_dir`: Where to write the files to
    - `output_prefix`: Name of bam file (without the extension
    - `mismatches`: Allowed mismatches
    - `fasta_genome`: genome fasta file. Must be indexed!
    - `params_aln`: extra parametrs for aln command
    - `params_sampe`: extra paarmeters for sampe execution
    - `params_samse`: extra parameters for samse execution
    - `samtools_cmd`: samtools command

    Return
    - `bamfile`: Output bam file name
    """
    # Run aln of bwa on both files
    sai1 = NamedTemporaryFile(dir=output_dir)
    call([bwa_cmd, 'aln', '-n', str(mismatches), params_aln,
          fasta_genome, fname1], stdout=sai1)
    if fname2:
        sai2 = NamedTemporaryFile(dir=output_dir)
        next_cmd = [bwa_cmd, 'aln', '-n', str(mismatches),
                    params_aln, fasta_genome, fname2]
        logging.info("Executing %s", ' '.join(next_cmd))
        call(next_cmd, stdout=sai2)
    # Run sampe on both sai files and convert to bam on the fly

    samtobam = [samtools_cmd, 'view', '-Sb', '-']
    bamsort = [samtools_cmd, 'sort', '-',
               "%s/%s"%(output_dir, output_prefix)]
    if fname2:
        next_cmd = ' '.join([bwa_cmd, 'sampe', params_sampe,
                       fasta_genome, sai1.name, sai2.name,
                       fname1, fname2] + ['|'] + samtobam + ['|'] + bamsort)
        logging.info("Executing %s"%next_cmd)
        call(next_cmd, shell=True)
    else: # Single end
        next_cmd = ' '.join(
            [bwa_cmd, 'samse'] + params_samse.split(' ') + \
                [fasta_genome, sai1.name, fname1] + \
            ['|'] + samtobam + ['|'] + bamsort)
        logging.info("Executing %s"%next_cmd)
        call(next_cmd, shell=True)
    bamname = "%s/%s.bam"%(output_dir, output_prefix)
    index_cmd = [samtools_cmd, 'index', bamname]
    logging.info("Indexing bam file %s"%' '.join(index_cmd))
    call(index_cmd)
    return bamname



def read_gtf(gtf_file, feature, identifier):
    """
    Read a GTF file and return a list in the length of the genome in which each
    position contains a list of features that overlap this position
    Arguments:
    - `gtf_file`: An open gtf_file
    - `feature`: the name of the feature to index
    - `identifier`: The identifier to use from column 8
    """
    # First initialize a dictionary
    pos_feat = defaultdict(lambda: defaultdict(set))
    # Get all the names of the features
    all_features = set(['~~intergenic', '~~antisense'])
    for line in csv.reader(gtf_file, delimiter='\t'):
        if line[2] != feature:
            continue
        ids_dict = {}
        for id_pair in line[8].strip().split(';'):
            try:
                k, v = id_pair.strip().split(' ')
            except ValueError:
                pass
            ids_dict[k] = v.replace('"','')
        fid = ids_dict[identifier]
        all_features.add(fid)
        # Change to 0-based coordinates and add this feature to all the
        # positions it convers
        for i in range(int(line[3])-1, int(line[4])):
            # Concat the strand tot he name of the chromosome
            pos_feat[line[0]+line[6]][i].add(fid)
    # Change the dictionary to list
    pos_feat_list = {}
    for chrom, data in pos_feat.items():
        maxpos = max(data.keys())
        list_of_sets = []
        for k in range(maxpos+1):
            list_of_sets.append(list(data[k]))
        pos_feat_list[chrom] = list_of_sets
    return pos_feat_list, all_features



def get_paired_pos(read, rev=False):
    """
    Given a read which is the positive of the pairs return a strand and
    start and end positions
    Arguments:
    - `read`: A read from pysam
    """
    strand = '+'
    if rev!=read.is_read2:
        strand = '-'
    fpos = read.pos
    tpos = read.tlen + fpos
    return strand, fpos, tpos

def get_single_pos(read, rev=False):
    """
    Given a read which is the positive of the pairs return a strand and
    start and end positions
    Arguments:
    - `read`: A read from pysam
    """
    strand = '+'
    if rev!=read.is_reverse:
        strand = '-'
    fpos = read.pos
    tpos = read.qlen + fpos
    return strand, fpos, tpos




def count_features(
    features_lists, samfile, overlap, rev=False, checkpoint=100000):
    """
    Go over the samfile and for each pair of reads find the features that
    overlap the fragment with at least 'overlap' nucleotides. Add 1 to the count
    of these features
    
    Arguments:
    - `features_lists`: The list of features returned from the read_gtf function
    - `samfile`: A pysam object
    - `overlap`: The minimal overlap between the feature and read
    - `rev`: reverse the strand of the read
    - `checkpoint`: Report every 100000 reads processed, set to None or False
                    for silencing
                    
    Return:
    - `fcounts`: A dictionary from gene name to number of reads mapped to the
                 gene. ~~antisense and ~~intergenic count the number of reads
                 that were not mapped to a gene and found antisense to a gene
                 or in intergenic region.
    """
    fcounts = defaultdict(int)
    counter = 0
    for read in samfile.fetch():
        if read.is_paired:
            if read.is_reverse or read.is_unmapped or\
                read.mate_is_unmapped or\
                read.is_reverse==read.mate_is_reverse or\
                not read.is_proper_pair:

                continue
        else: # single end
            if  read.is_unmapped:
                continue
        # Take only the forward mate
        counter += 1
        if checkpoint and counter%checkpoint==0:
            sys.stderr.write("Processed %i fragments\n"%counter)
        try:
            chrname = samfile.getrname(read.tid)
        except ValueError:
            sys.stderr.write(str(read)+"\n")
        # Get the positions of the fragment
        if read.is_paired:
            strand, fpos, tpos = get_paired_pos(read, rev=rev)
        else:
            strand, fpos, tpos = get_single_pos(read, rev=rev)
        
        # Count the number of times a feature intersects with the fragmen
        rcounts = defaultdict(int)
        for fset in features_lists[chrname+strand][fpos:tpos]:

            for el in fset:
                rcounts[el] += 1
        # Go over the list of features, if the number of counts is above the
        # Threshold add 1 to the count of this feature
        is_counted = False
        for feature, counts in rcounts.items():
            if counts >= overlap:
                fcounts[feature] += 1
                is_counted = True
        if not is_counted:
            # Test if antisense
            rev_str = '-'
            if strand == '-':
                rev_str = '+'
            rev_counts = defaultdict(int)
            for fset in features_lists[chrname+rev_str][fpos:tpos]:
                for el in fset:
                    rev_counts[el] += 1
            is_antis = False
            for feature, counts in rev_counts.items():
                if counts >= overlap:
                    is_antis = True
                    break
            if is_antis:
                fcounts['~~antisense'] += 1
            else:
                fcounts['~~intergenic'] += 1
    return fcounts


def generate_wig(samfile, rev=False, first_pos=False):
    """
    Go over the samfile and return two histograms (for + and - strands) of
    coverage
    
    Arguments:
    - `samfile`: A pysam object
    - `rev`: reverse the strand of the read
    - `first_pos`: Count only the first position of each read
    """
    # Build the structure of the dictionary chromosome->strand->list of 0
    coverage = {}
    for i, rfg in enumerate(samfile.references):
        rlen = samfile.lengths[i]
        coverage[rfg] = {'-':[0] * rlen, '+':[0] * rlen}
    for read in samfile.fetch():
        if read.is_paired:
            if read.is_reverse or read.is_unmapped or\
                read.mate_is_unmapped or\
                read.is_reverse==read.mate_is_reverse or\
                not read.is_proper_pair:
                continue
        else: # single end
            if  read.is_unmapped:
                continue
        # Take only the forward mate
        try:
            chrname = samfile.getrname(read.tid)
        except ValueError:
            logging.warn("Read has no valid chr name %s"%(str(read)))
            continue
        # Get the positions of the fragment
        if read.is_paired:
            strand, fpos, tpos = get_paired_pos(read, rev=rev)
        else:
            strand, fpos, tpos = get_single_pos(read, rev=rev)
        rrange = range(fpos, tpos)
        if first_pos:
            if strand == '+':
                rrange = [fpos]
            else:
                rrange = [tpos]
        for i in rrange:
            try:
                coverage[chrname][strand][i] += 1
            except IndexError:
                logging.warn("IndexError: trying to set index %d on chr %s, bu length is only %d"%(i, chrname, len(coverage[chrname][strand])))
    return coverage


def print_wiggle(coverage, title, description, outf):
    """
    Print the coverage into an open wiggle file
    Arguments:
    - `coverage`: returned from generate_wig
    - `title`: Title of wig track
    - `description`: Description of track
    - `outf`: Open file to write to
    """
    for chr_name in coverage:
        outf.write('track type=wiggle_0 name=%s_PLUS description="%s PLUS" visibility=full color=0,0,255\n'%(
            title, description))
        outf.write("fixedStep chrom=%s start=1 step=1\n"%chr_name)
        for c in coverage[chr_name]['+']:
            outf.write("%d\n"%c)
        

        outf.write('track type=wiggle_0 name=%s_MINUS description="%s MINUS" visibility=full color=255,0,0\n'%(
            title, description))
        outf.write("fixedStep chrom=%s start=1 step=1\n"%chr_name)
        for c in coverage[chr_name]['-']:
            outf.write("%d\n"%c)


# Functions for mapping of chimeric fragments
def read_transcripts(trans_gff, feature='exon', identifier='gene_id'):
    """
    Read the transcripts, return a dictionary from transcript name to
    position
    Arguments:
    - `trans_gff`: A gff file of transcripts
    - `feature`: the name of the feature to index
    - `identifier`: The identifier to use from column 8
    """
    tus = {}
    for line in csv.reader(open(trans_gff), delimiter='\t'):
        if line[2] != feature:
            continue
        ids_dict = {}
        for id_pair in line[8].strip().split(';'):
            try:
                k, v = id_pair.strip().split(' ')
            except ValueError:
                pass
            ids_dict[k] = v.replace('"','')
        fid = ids_dict[identifier]
        tus[fid] = (int(line[3])-1, int(line[4]), line[6])
    return tus


def get_unmapped_reads(samfile, outfile1, outfile2, length, maxG, rev=False):
    """
    Get the list of unmapped paired reads and write the reads (mate 1 and 2) to
    the fastq files outfile1 and outfile2. The names of the reads is the same
    (assume equal in bam file)
    If rev is set assume first read is the reverse complement and reverse
    complement it, put it as read 2 and treat the second read as read 1.
    Can handle single-end as well
    Arguments:
    - `samfile`: Open Samfile object
    - `outfile1`: Open fastq file for reads 1
    - `outfile2`: Open fastq file for reads 2
    - `length`: Write the first X nt of the sequences
    - `maxG`: Maximal fraction of G's in any of the reads
    - `rev`: Reads are reverse complement (Livny's protocol). Has no influence
             on single-end reads
    """
    for read in samfile.fetch(until_eof=True):
        if (not read.is_paired) and read.is_unmapped:
            if read.is_reverse:
                # This can't happen
                continue
            cseq = read.seq
            cqual = read.qual
            if cseq.count('G', 0, length) >= int(maxG*length) or\
                    cseq.count('G', -length) >= int(maxG*length):
                continue
            outfile1.write("@%s\n%s\n+\n%s\n"%(
                    read.qname, cseq[:length],
                    cqual[:length]))
            outfile2.write("@%s\n%s\n+\n%s\n"%(
                    read.qname, cseq[-length:],
                    cqual[-length:]))
            continue
        if (read.is_unmapped or read.mate_is_unmapped or\
                (not read.is_proper_pair)) and read.is_paired:
            if read.is_read1==rev:
                ouf = outfile2
                outseq = Seq(read.seq)
                outqual = read.qual[-length:]
                # Reverse complement the read if it haven't been
                # done in the bam file. Otherwise, do nothing
                if not read.is_reverse:
                    outseq = outseq.reverse_complement()
                    outqual = read.qual[::-1][-length:]
                if (str(outseq).count('C')>=int(maxG*length)):
                    continue
                outseq = str(outseq[-length:])
            else: # First read in the fragment
                ouf = outfile1
                outseq = Seq(read.seq)
                outqual = read.qual[:length]
                if read.is_reverse:
                    outseq = outseq.reverse_complement()
                    outqual = read.qual[::-1][:length]
                if outseq.count('G') >= int(maxG*length):
                    continue
                outseq = str(outseq[:length])
            ouf.write("@%s\n%s\n+\n%s\n"%(read.qname, outseq, outqual))


def get_XA_mapping(tags, max_mm=None):
    """
    Return a list of positions from the XA sam file data, these are additional
    mappings
    Arguments:
    - `tags`: The tags argument
    """
    tlist = []            
    for tpair in tags:
        if tpair[0] == 'XA':
            for xadat in tpair[1].split(';')[:-1]:
                alt_dat = xadat.split(',')
                if max_mm is None or int(alt_dat[-1])<=max_mm:
                    tlist.append(alt_dat)
    return tlist


def get_NM_number(tags):
    """
    Return the number of mismatches found in the NM tag
    Arguments:
    - `tags`: The tags argument
    """
    for tpair in tags:
        if tpair[0] == 'NM':
            return tpair[1]
    return 10


def test_concordance(
    read1, read2, maxdist, chrnames_bam, trans_gff=None, remove_self=False):
    """
    Test if the two reads can be concordant and not ligated
    Update:
    test all the XA, if one combination is concordant return True
    Arguments:
    - `read1`: read 1 object
    - `read2`: read 2 object
    - `maxdist`: Maximal distance to consider concordance if not on the same
                 transcript and have same orientation (possibly self ligation)
    - `chrnames_bam`: A list of chr names as they appear in the bam file.
                      can be generated from Samfile.getrname() function
    - `trans_gff`: A dict TU->(from, to, strand)
    - `remove_self`: Remove pairs that have the same orientation but different
                     order i.e. r1--->r2---> instead of r2--->r1--->
    """
    def is_conc(str1, str2, pos1, pos2, chr1, chr2):
        """
        """
        if str1 != str2:
            return False
        if chr1 != chr2:
            return False
        if abs(pos2-pos1)<maxdist and ((pos1>pos2)==str1 or remove_self):
            return True
        if trans_gff:
            in_trans = False
            for tu_pos in trans_gff.values():
                if (tu_pos[2]=='-')==str1:
                    if tu_pos[0]<=pos1<=tu_pos[1] and tu_pos[0]<=pos2<=tu_pos[1]:
                        in_trans = True
                        break
            if in_trans and ((pos1>pos2)==str1 or remove_self):
                return True
        return False
                        
    
    # read1 is the reverse of the real read. If both directions are the same
    # and distance is short they can be concordant
    
    if read1.is_reverse == read2.is_reverse and read1.tid==read2.tid:
        if is_conc(
            read1.is_reverse, read2.is_reverse, read1.pos, read2.pos,
            read1.tid, read2.tid):
            return True
    r1_XA = get_XA_mapping(read1.tags)
    r2_XA = get_XA_mapping(read2.tags)
    if r1_XA:
        for altp in r1_XA:
            is_rev1 = (altp[1][0] == '-')
            pos1 = abs(int(altp[1]))
            if is_conc(
                is_rev1, read2.is_reverse, pos1, read2.pos, altp[0],
                chrnames_bam[read2.tid]):
                return True
            for altp2 in r2_XA:
                is_rev2 = (altp2[1][0] == '-')
                pos2 = abs(int(altp2[1]))
                if is_conc(is_rev1, is_rev2, pos1, pos2, altp[0], altp2[0]):
                    return True
    if r2_XA:
        for altp2 in r2_XA:
            is_rev2 = (altp2[1][0] == '-')
            pos2 = abs(int(altp2[1]))
            if is_conc(read1.is_reverse, is_rev2, read1.pos, pos2, altp2[0], chrnames_bam[read1.tid]):
                return True
        
    return False

def read_bam_file(bamfile, chrnames_bam, max_NM=0):
    """
    Given a Samfile object of a single mapped file, return the reads in the file
    If there are more than one mapping to a read return the first in the genome.
    Test if the read has less than (or equals to) number of allowed mismatches
    Arguments:
    - `bamfile`: A Samfile object
    - `chrnames_bam`: The names of the chromosomes
    - `max_NM`: Maximal number of mismatches
    """
    read_objects = {}
    for read in bamfile.fetch():
        if not read.is_unmapped:
            nm_num = get_NM_number(read.tags)
            if nm_num > max_NM:
                continue
            # If there are multiple hits, choose the one with the smallest coor
            alt_list = get_XA_mapping(read.tags, max_NM)
            min_pos = read.pos
            min_is_rev = read.is_reverse
            for al in alt_list:
                apos = int(al[1][1:])
                if apos < min_pos:
                    min_pos = apos
                    min_is_rev = al[1][0]=='-'
            # If changed, add the read one to the XA tag
            tags = read.tags
            if read.pos !=min_pos:
                for xt in tags:
                    if xt[0] == 'XA':
                        xaval = xt[1]
                        tags.remove(xt)
                        strs = '+'
                        if read.is_reverse:
                            strs = '-'
                        tags.append(('XA', '%s,%s%d,%s,%d;'%(
                                    chrnames_bam[read.tid],
                                    strs, read.pos,
                                    read.cigarstring, nm_num) +xaval))
                read.tags = tags
            read.pos = min_pos
            read.is_reverse = min_is_rev
            read_objects[read.qname] = read
    return read_objects


def write_reads_table(
    outfile, read1_reads, read2_reads, chrnames_EC, chrnames_bam, maxdist,
    remove_self, trans_gff=None):
    """
    Read the lists of reads and print a list of chimeric fragments after
    removing concordant reads
    Arguments:
    - `outfile`: Print the reads positions to this open file
    - `read1_reads`: A dictionary of reads from side 1
    - `read2_reads`: A dictionary of reads from side 2, keys of 1 and 2 should
                     match
    - `chrnames_EC`: A list of chromosome names for output, should be EcoCyc
                     names for downstream analysis
    - `chrnames_bam`: A list of chromosome names in the bam file
    - `maxdist`: Maximal distance between concordant reads
    - `remove_self`: Remove circular RNAs
    - `trans_gff`: A dictionary with transcripts positions, optional
    """
    

    for rname in read1_reads:
        if rname not in read2_reads:
            continue
        # If the two reads share at least one gene they are excluded
        if test_concordance(
            read1_reads[rname], read2_reads[rname], trans_gff, maxdist,
            chrnames_bam, remove_self):
            continue
        read1_chrn = chrnames_EC[read1_reads[rname].tid]
        read2_chrn = chrnames_EC[read2_reads[rname].tid]
        if not read2_reads[rname].is_reverse:
            end2_pos = read2_reads[rname].pos+read2_reads[rname].qlen-1
            end2_str = '+'
        else:
            end2_pos = read2_reads[rname].pos
            end2_str = '-'
        if read1_reads[rname].is_reverse:
            end1_pos = read1_reads[rname].pos+read1_reads[rname].qlen-1
            end1_str = '-'
        else:
            end1_pos = read1_reads[rname].pos
            end1_str = '+'
        outfile.write(
            "%s\t%d\t%s\t%s\t%d\t%s\t%s\n"%(
                read1_chrn, end1_pos+1, end1_str, read2_chrn, end2_pos+1,
                end2_str, rname))


def read_reads_table(reads_in, seglen):
    """
    Read a reads table and count the number of times each pair of segments
    appears in a chimeric fragment.
    Arguments:
    - `reads_in`: The table (tab delimited) of reads
    - `seglen`: The length of the segment
    """
    region_interactions = defaultdict(lambda:defaultdict(list))
    region_ints_as1 = defaultdict(int)
    region_ints_as2 = defaultdict(int)
    total_interactions = 0
    for line in reads_in:
        end1_chrn, end1_pos1, end1_str, end2_chrn, end2_pos1, end2_str, _ =\
            line.strip().split()
        end1_pos = int(end1_pos1)-1
        end2_pos = int(end2_pos1)-1
        end1_seg = (end1_pos/seglen)*seglen
        end2_seg = (end2_pos/seglen)*seglen
        total_interactions += 1
        region_interactions[(end1_seg, end1_str, end1_chrn)]\
            [(end2_seg, end2_str, end2_chrn)].append((end1_pos, end2_pos))
        region_ints_as1[(end1_seg, end1_str, end1_chrn)] += 1
        region_ints_as2[(end2_seg, end2_str, end2_chrn)] += 1
    return (
        region_interactions, region_ints_as1, region_ints_as2,
        total_interactions)



def minpv_regions(
    reg1, reg2, r_int, t_int_as1, t_int_as2, tot_int, f_int, seglen, maxsegs,
    maxdist, min_odds):
    """
    return the regions that have minimal p-value. Exhaustive search
    Arguments:
    - `reg1`: Region1 seed
    - `reg2`: Region2 seed
    - `r_int`: regions interactions double dictionary
    - `t_int_as1`: Total interactions of every region as region 1
    - `t_int_as2`: As above for the second read
    - `tot_int`: Total interractions
    - `f_int`: interacting regions that were used
    - `seglen`: Length of segment (usually 100)
    - `maxsegs`: Maximal number of neighbouring segment to join
    - `maxdist`: Remove interactions that are this close to each other
    - `min_odds`: Minimal odds-ratio to test
    """
    maxpv = 2
    maxpars = [0] * 9
    corr = 0
    for l1 in range(maxsegs):
        for l2 in range(maxsegs):
            for i1 in range(l1+1):
                for i2 in range(l2+1):
                    has_former = False
                    # Reduce from the total number of interactions the
                    # interactions near the sites because they are
                    # excluded from the analysis
                    region_reduce = 0
                    from_red1 = reg1[0]-(i1*seglen)-maxdist
                    to_red1 = reg1[0]+(l1*seglen)+maxdist
                    for red1_r in range(from_red1, to_red1, seglen):
                        region_reduce += t_int_as2[(red1_r, reg1[1], reg1[2])]
                    from_red2 = reg2[0]-(i2*seglen)-maxdist
                    to_red2 = reg2[0]+(l2*seglen)+maxdist
                    for red2_r in range(from_red2, to_red2, seglen):
                        region_reduce += t_int_as1[(red2_r, reg2[1], reg2[2])]
                    int_sum = 0
                    s1_sum = 0
                    s2_sum = 0
                    for r2 in range(l2+1):
                        real_r2 = reg2[0]+((r2-i2)*seglen)
                        s2_sum += t_int_as2[(real_r2, reg2[1], reg2[2])]
                    for r1 in range(l1+1):
                        real_r1 = reg1[0]+((r1-i1)*seglen)
                        s1_sum += t_int_as1[(real_r1, reg1[1], reg1[2])]
                        for r2 in range(l2+1):
                            real_r2 = reg2[0]+((r2-i2)*seglen)
                            if (real_r1, reg1[1], reg1[2], real_r2,reg2[1], reg2[2]) in f_int:
                                has_former = True
                                continue
                            int_sum += \
                                len(r_int[(real_r1, reg1[1], reg1[2])]\
                                        [(real_r2, reg2[1], reg2[2])])
                        
                    if has_former:
                        continue
                    # Set the values of the 2x2 contingency table a b c d:
                    #   a   b
                    #   c   d
                    a = int_sum
                    b = s1_sum - int_sum
                    c = s2_sum - int_sum
                    d = tot_int - s1_sum - s2_sum + int_sum - region_reduce
                    corr += 1
                    if b==0 or c==0:
                        odds = np.inf
                    else:
                        odds = (float(a)*d)/(float(b)*c)
                    if odds < min_odds:
                        continue
                    odds, pv = fisher_exact(
                        [[a, b], [c, d]], alternative='greater')
                    if pv <= maxpv:
                        replace = False
                        if pv == maxpv:
                            # Both 0 probably, take the one with more
                            # interactions. If equal take the narrow one
                            if maxpars[4] < int_sum:
                                replace = True
                            else:
                                if (l1<=maxpars[0] and l2<maxpars[1]) or\
                                    (l2==maxpars[1] and l1<maxpars[0]):
                                    replace = True
                        else:
                            replace = True
                        if replace:
                            maxpv = pv 
                            maxpars = (l1, l2, i1, i2, int_sum, odds, b, c, d)
    return (maxpv*(corr or 1), maxpars[4], maxpars[5],
            reg1[0]-maxpars[2]*seglen,
            reg1[0]+(maxpars[0]-maxpars[2]+1)*seglen,
            reg2[0]-maxpars[3]*seglen,
            reg2[0]+(maxpars[1]-maxpars[3]+1)*seglen,
            maxpars[6], maxpars[7], maxpars[8])


def read_targets(tarfile):
    """
    Return a tuple of targets
    Arguments:
    - `tarfile`: A tab-del file with EcoCyc names of sRNA and target
    """
    tars = []
    for line in open(tarfile):
        tars.append(tuple(line.strip().split()[:2]))
    return tars

def read_singles(singles_file):
    """
    Read the table of reads per gene and return a dictionary
    """
    counts = {}
    for line in open(singles_file):
        spl = line.strip().split()
        counts[spl[0]] = sum([int(k) for k in spl[1:]])
    return counts


def read_annotations(refseq_dir, an_ext = ('.ptt.gz', '.rnt.gz')):
    """
    Read the annotations from rnt and ptt files and return a dictionary
    Arguments:
    - `refseq_dir`: The Refseq dictionary
    - `an_ext`: Extensions of annotation files (iterable)
    """
    annotations = {}
    ec_files = []
    for ext in an_ext:
        ec_files.extend(glob.glob("%s/*%s"%(refseq_dir, ext)))
    for fin in ec_files:
        fo = gzip.open(fin)
        for row in csv.reader(fo, delimiter='\t'):
            try:
                annotations[row[4]] = row[8]
            except IndexError:
                pass
    return annotations


def get_genes_dict(ec_dir, pad=100):
    """
    Return a dictionary from genomic position to annotation based on EcoCyc
    Arguments:
    - `ec_dir`: EcoCyc directory
    - `pad`: Estimated UTR length
    """
    pos_maps, uid_gene = ecocyc_parser.get_mapping(ec_dir, pad)
    pos_maps_lists = defaultdict(dict)
    for chrn, pos_data in pos_maps.items():
        for k, v in pos_data.items():
            try:
                cn1 = uid_gene[v[0]]['COMMON-NAME']
            except KeyError:
                cn1 = v[0]

            if len(v)>2 and (v[2]=='IGR' or v[2]=='TU' or v[2]=='TU_AS'):
                try:
                    cn2 = uid_gene[v[1]]['COMMON-NAME']
                except KeyError:
                    cn2 = ''
                outlist = list(v[:2])+[cn1, cn2, v[2]]
            else:
                outlist = list(v[:1])+[cn1]
                if len(v)>1:
                    outlist.append(v[1])
            pos_maps_lists[chrn][k] = '.'.join(outlist)
    return pos_maps_lists


def list_of_genes(
    r1_region_from, r1_region_to, r1_str, r1_chrn, r2_region_from, r2_region_to,
    r2_str, r2_chrn, region_interactions, genes_dict, seglen, rlen):
    """
    Return a list of genes that intersect with the given regions.
    Return the minimal and maximal positions of reads in the regions as well.
    The genes are sorted according to the number of reads starting in each
    position
    Arguments:
    - `r1_region_from`: the first position of r1 
    - `r1_region_to`: The end of r1
    - `r1_str`: r1 strand
    - `r1_chrn`: r1 chromosome
    - `r2_region_from`: region 2 from
    - `r2_region_to`: region 2 to
    - `r2_str`: r2 strand
    - `r2_chrn`: r2 chromosome
    - `region_interactions`: A double dictionary region1->region2->[(p1, p2)]
    - `genes_dict`: dictionary of genes
    - `seglen`: length of segments
    - `rlen`: length of read
    """
    cdict_r1 = defaultdict(int)
    cdict_r2 = defaultdict(int)
    min1_pos = np.inf
    max1_pos = 0
    min2_pos = np.inf
    max2_pos = 0
    for r1_reg in range(r1_region_from, r1_region_to, seglen):
        for r2_reg in range(r2_region_from, r2_region_to, seglen):
            for (r1p, r2p) in region_interactions[(r1_reg, r1_str, r1_chrn)]\
                    [(r2_reg, r2_str, r2_chrn)]:
                min1_pos = min(min1_pos, r1p)
                max1_pos = max(max1_pos, r1p)
                min2_pos = min(min2_pos, r2p)
                max2_pos = max(max2_pos, r2p)
                if r1_str == '+':
                    drange = range(max(r1p-rlen,0), r1p)
                else:
                    drange = range(r1p, r1p+rlen)
                for i in drange:
                    cdict_r1[genes_dict[r1_chrn][(i, r1_str)]] += 1
                if r2_str == '+':
                    drange = range(r2p, r2p+rlen)
                else:
                    drange = range(max(r2p-rlen,0), r2p)
                for i in drange:
                    cdict_r2[genes_dict[r2_chrn][(i, r2_str)]] += 1
    genes1_list = sorted(cdict_r1, key=cdict_r1.get, reverse=True)
    genes2_list = sorted(cdict_r2, key=cdict_r2.get, reverse=True)
    return genes1_list, genes2_list, min1_pos, min2_pos, max1_pos, max2_pos


def report_interactions(
    region_interactions, outfile, interacting_regions, seglen, ec_dir, ec_chrs,
    refseq_dir, targets_file, single_counts, shuffles, RNAup_cmd, servers, rlen,
    est_utr_lens):
    """
    Report the interactions with additional data such as genes in region, if
    it's a known target, number of single fragments count, binding energy
    Arguments:
    - `region_interactions`: The double dictionary containing the reads
    - `outfile`: An open file to write the data to
    - `interacting_regions`: A list of interacting regions as tuples
    - `seglen`: Segment length
    - `ec_dir`: Directory of EcoCyc flatfiles
    - `ec_chrs`: The order of chromosomes in EcoCyc
    - `refseq_dir`: The RefSeq directory to get the gene descriptions from
    - `targets_file`: A file with sRNA-target in EcoCyc IDs
    - `single_counts`: A file with single counts, take the sum of each row
    - `shuffles`: Number of shuffles to compute empirical p-value
    - `RNAup_cmd`: command line of RNAup
    - `servers`: Number of CPUs or a list of servers to use.
    - `rlen`: Length of reads
    - `est_utr_lens`: Estimated lengths of UTRs when data is not available in
                      EcoCyc
    """
    targets = read_targets(targets_file)
    singles = read_singles(single_counts)
    desc = read_annotations(refseq_dir)
    genes_dict = get_genes_dict(ec_dir, est_utr_lens)
    rnup = 
